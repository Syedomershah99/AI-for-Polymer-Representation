{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polymer Representation Analysis\n",
    "\n",
    "This notebook implements a comprehensive polymer representation and clustering analysis:\n",
    "1. **Data Cleaning**: Canonicalize SMILES, remove invalid entries, deduplicate\n",
    "2. **Representations**: Morgan fingerprints, MACCS keys, RDKit descriptors, Transformer embeddings\n",
    "3. **Unsupervised Clustering**: K-means (with elbow/silhouette), Tanimoto-distance clustering\n",
    "4. **Validation**: ARI/NMI against polymer_class\n",
    "5. **Supervised Baselines**: Stratified 5-fold cross-validation\n",
    "6. **Interpretability**: Representative polymers per cluster with structure visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip -q install rdkit umap-learn tqdm scikit-learn pandas numpy matplotlib plotly transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, MACCSkeys, Draw\n",
    "from rdkit import DataStructs\n",
    "from rdkit.ML.Cluster import Butina\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, adjusted_rand_score, normalized_mutual_info_score,\n",
    "    accuracy_score, f1_score, classification_report\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Clean Dataset\n",
    "### 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create plots folder\nimport os\nos.makedirs('plots', exist_ok=True)\nprint(\"Created plots/ folder for saving figures\")\n\n# Adjust path based on environment (Colab vs local)\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DATA_PATH = \"/content/drive/MyDrive/PI1070.csv\"\nexcept:\n    DATA_PATH = \"PI1070.csv\"  # Local path\n\ndf_raw = pd.read_csv(DATA_PATH)\nprint(f\"Loaded dataset: {df_raw.shape}\")\nprint(f\"Columns: {df_raw.columns[:15].tolist()}...\")\ndf_raw.head(3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Canonicalize SMILES and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_smiles(smiles):\n",
    "    \"\"\"\n",
    "    Canonicalize SMILES string using RDKit.\n",
    "    Returns canonical SMILES or None if invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        smiles = str(smiles).strip()\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def clean_dataset(df):\n",
    "    \"\"\"\n",
    "    Clean the dataset:\n",
    "    1. Canonicalize SMILES\n",
    "    2. Remove invalid entries\n",
    "    3. Deduplicate based on canonical SMILES\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Step 1: Canonicalize SMILES\n",
    "    print(\"Canonicalizing SMILES...\")\n",
    "    tqdm.pandas(desc=\"Canonicalizing\")\n",
    "    df['smiles_canonical'] = df['smiles'].progress_apply(canonicalize_smiles)\n",
    "    \n",
    "    # Step 2: Remove invalid SMILES\n",
    "    n_before = len(df)\n",
    "    df = df[df['smiles_canonical'].notna()].reset_index(drop=True)\n",
    "    n_invalid = n_before - len(df)\n",
    "    print(f\"Removed {n_invalid} invalid SMILES entries\")\n",
    "    \n",
    "    # Step 3: Deduplicate based on canonical SMILES\n",
    "    n_before = len(df)\n",
    "    df = df.drop_duplicates(subset='smiles_canonical', keep='first').reset_index(drop=True)\n",
    "    n_duplicates = n_before - len(df)\n",
    "    print(f\"Removed {n_duplicates} duplicate SMILES entries\")\n",
    "    \n",
    "    # Create mol objects\n",
    "    df['mol'] = df['smiles_canonical'].apply(lambda s: Chem.MolFromSmiles(s))\n",
    "    \n",
    "    print(f\"Final dataset size: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "df = clean_dataset(df_raw)\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  - Total polymers: {len(df)}\")\n",
    "print(f\"  - Has polymer_class: {'polymer_class' in df.columns}\")\n",
    "if 'polymer_class' in df.columns:\n",
    "    print(f\"  - Unique classes: {df['polymer_class'].nunique()}\")\n",
    "    print(f\"  - Class distribution:\\n{df['polymer_class'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Polymer Representations\n",
    "\n",
    "We compute four types of representations:\n",
    "1. **Morgan Fingerprints** (ECFP) - circular fingerprints capturing local atom environments\n",
    "2. **MACCS Keys** - 166 predefined structural keys (standard substructure patterns)\n",
    "3. **RDKit Descriptors + MACCS Motifs** - physicochemical descriptors with substructure information\n",
    "4. **Transformer Embeddings** - pretrained polyBERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Morgan Fingerprints (ECFP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_morgan_fingerprints(mols, radius=2, n_bits=2048):\n",
    "    \"\"\"\n",
    "    Compute Morgan fingerprints (ECFP) for a list of molecules.\n",
    "    Returns both RDKit bitvectors and numpy arrays.\n",
    "    \"\"\"\n",
    "    fps_rdkit = []  # For Tanimoto calculations\n",
    "    fps_array = []  # For ML models\n",
    "    \n",
    "    for mol in tqdm(mols, desc=\"Computing Morgan FPs\"):\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "        fps_rdkit.append(fp)\n",
    "        \n",
    "        arr = np.zeros((n_bits,), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        fps_array.append(arr)\n",
    "    \n",
    "    return fps_rdkit, np.vstack(fps_array)\n",
    "\n",
    "fps_morgan_rdkit, X_morgan = compute_morgan_fingerprints(df['mol'].tolist())\n",
    "print(f\"Morgan fingerprints shape: {X_morgan.shape}\")\n",
    "print(f\"Sparsity: {100 * (X_morgan == 0).sum() / X_morgan.size:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MACCS Keys (166 Standard Substructure Patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_maccs_keys(mols):\n",
    "    \"\"\"\n",
    "    Compute MACCS keys (166-bit structural keys) for molecules.\n",
    "    MACCS keys are standard substructure patterns widely used in cheminformatics.\n",
    "    \"\"\"\n",
    "    fps_rdkit = []\n",
    "    fps_array = []\n",
    "    \n",
    "    for mol in tqdm(mols, desc=\"Computing MACCS keys\"):\n",
    "        fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "        fps_rdkit.append(fp)\n",
    "        \n",
    "        arr = np.zeros((167,), dtype=np.int8)  # MACCS has 167 bits (0-166)\n",
    "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        fps_array.append(arr)\n",
    "    \n",
    "    return fps_rdkit, np.vstack(fps_array)\n",
    "\n",
    "fps_maccs_rdkit, X_maccs = compute_maccs_keys(df['mol'].tolist())\n",
    "print(f\"MACCS keys shape: {X_maccs.shape}\")\n",
    "print(f\"Active bits per molecule (mean): {X_maccs.sum(axis=1).mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 RDKit Descriptors + MACCS Motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rdkit_descriptors(mols):\n",
    "    \"\"\"\n",
    "    Compute all RDKit molecular descriptors.\n",
    "    Handles NaN/Inf values by imputing with column medians.\n",
    "    \"\"\"\n",
    "    descriptor_names = [d[0] for d in Descriptors._descList]\n",
    "    descriptor_fns = [d[1] for d in Descriptors._descList]\n",
    "    \n",
    "    results = []\n",
    "    for mol in tqdm(mols, desc=\"Computing RDKit descriptors\"):\n",
    "        vals = []\n",
    "        for fn in descriptor_fns:\n",
    "            try:\n",
    "                v = fn(mol)\n",
    "                if v is None or (isinstance(v, float) and (np.isnan(v) or np.isinf(v))):\n",
    "                    v = np.nan\n",
    "            except Exception:\n",
    "                v = np.nan\n",
    "            vals.append(v)\n",
    "        results.append(vals)\n",
    "    \n",
    "    X = np.array(results, dtype=np.float32)\n",
    "    \n",
    "    # Remove columns that are all NaN\n",
    "    valid_mask = ~np.all(np.isnan(X), axis=0)\n",
    "    X = X[:, valid_mask]\n",
    "    kept_names = [n for n, keep in zip(descriptor_names, valid_mask) if keep]\n",
    "    \n",
    "    # Impute remaining NaNs with column medians\n",
    "    col_median = np.nanmedian(X, axis=0)\n",
    "    nan_idx = np.where(np.isnan(X))\n",
    "    X[nan_idx] = np.take(col_median, nan_idx[1])\n",
    "    \n",
    "    return X, kept_names\n",
    "\n",
    "X_rdkit_desc, rdkit_desc_names = compute_rdkit_descriptors(df['mol'].tolist())\n",
    "print(f\"RDKit descriptors shape: {X_rdkit_desc.shape}\")\n",
    "\n",
    "# Combine RDKit descriptors with MACCS keys as motif representation\n",
    "X_desc_maccs = np.hstack([X_rdkit_desc, X_maccs.astype(np.float32)])\n",
    "print(f\"RDKit + MACCS combined shape: {X_desc_maccs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Transformer Embeddings (polyBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transformer_embeddings(smiles_list, model_name=\"kuelumbus/polyBERT\", \n",
    "                                    batch_size=64, max_len=256, pooling=\"cls\"):\n",
    "    \"\"\"\n",
    "    Compute transformer embeddings using a pretrained SMILES encoder.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(smiles_list), batch_size), desc=\"Computing embeddings\"):\n",
    "            batch = smiles_list[i:i+batch_size]\n",
    "            \n",
    "            tokens = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            output = model(**tokens)\n",
    "            hidden = output.last_hidden_state\n",
    "            \n",
    "            if pooling == \"cls\":\n",
    "                emb = hidden[:, 0, :]\n",
    "            else:  # mean pooling\n",
    "                mask = tokens[\"attention_mask\"].unsqueeze(-1)\n",
    "                emb = (hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "            \n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "X_transformer = compute_transformer_embeddings(df['smiles_canonical'].tolist())\n",
    "print(f\"Transformer embeddings shape: {X_transformer.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Standardize Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize continuous representations (not binary fingerprints)\n",
    "scaler_rdkit = StandardScaler()\n",
    "X_rdkit_desc_scaled = scaler_rdkit.fit_transform(X_rdkit_desc)\n",
    "\n",
    "scaler_desc_maccs = StandardScaler()\n",
    "X_desc_maccs_scaled = scaler_desc_maccs.fit_transform(X_desc_maccs)\n",
    "\n",
    "scaler_transformer = StandardScaler()\n",
    "X_transformer_scaled = scaler_transformer.fit_transform(X_transformer)\n",
    "\n",
    "# Summary of all representations\n",
    "print(\"\\n=== Representation Summary ===\")\n",
    "print(f\"Morgan FP (binary):        {X_morgan.shape}\")\n",
    "print(f\"MACCS Keys (binary):       {X_maccs.shape}\")\n",
    "print(f\"RDKit Desc (scaled):       {X_rdkit_desc_scaled.shape}\")\n",
    "print(f\"RDKit+MACCS (scaled):      {X_desc_maccs_scaled.shape}\")\n",
    "print(f\"Transformer (scaled):      {X_transformer_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unsupervised Clustering\n",
    "\n",
    "### 3.1 K-Means Clustering on Transformer Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def kmeans_analysis(X, k_range=range(2, 16), random_state=42):\n    \"\"\"\n    Perform K-means clustering with elbow curve and silhouette analysis.\n    \"\"\"\n    inertias = []\n    silhouettes = []\n    \n    for k in tqdm(k_range, desc=\"K-means analysis\"):\n        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n        labels = kmeans.fit_predict(X)\n        \n        inertias.append(kmeans.inertia_)\n        silhouettes.append(silhouette_score(X, labels))\n    \n    return list(k_range), inertias, silhouettes\n\nk_values, inertias, silhouettes = kmeans_analysis(X_transformer_scaled)\n\n# Plot elbow curve and silhouette scores\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Elbow curve\naxes[0].plot(k_values, inertias, 'bo-', linewidth=2, markersize=8)\naxes[0].set_xlabel('Number of Clusters (k)', fontsize=12)\naxes[0].set_ylabel('Inertia (Within-cluster sum of squares)', fontsize=12)\naxes[0].set_title('Elbow Curve for K-Means', fontsize=14)\naxes[0].grid(True, alpha=0.3)\n\n# Silhouette scores\naxes[1].plot(k_values, silhouettes, 'ro-', linewidth=2, markersize=8)\nbest_k = k_values[np.argmax(silhouettes)]\naxes[1].axvline(x=best_k, color='g', linestyle='--', label=f'Best k={best_k}')\naxes[1].set_xlabel('Number of Clusters (k)', fontsize=12)\naxes[1].set_ylabel('Silhouette Score', fontsize=12)\naxes[1].set_title('Silhouette Analysis', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('plots/01_kmeans_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nBest k by silhouette score: {best_k} (score: {max(silhouettes):.4f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit final K-means with optimal k (or use number of known classes if available)\n",
    "if 'polymer_class' in df.columns:\n",
    "    n_classes = df['polymer_class'].nunique()\n",
    "    k_final = n_classes  # Use number of true classes for comparison\n",
    "else:\n",
    "    k_final = best_k\n",
    "\n",
    "print(f\"Using k={k_final} for final K-means clustering\")\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)\n",
    "df['kmeans_cluster'] = kmeans_final.fit_predict(X_transformer_scaled)\n",
    "\n",
    "print(f\"K-means cluster distribution:\")\n",
    "print(df['kmeans_cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tanimoto-Distance Clustering (Morgan & MACCS)\n",
    "\n",
    "#### 3.2.1 Fine-Grained Butina Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def butina_cluster(fps, cutoff=0.3):\n    \"\"\"\n    Butina clustering using Tanimoto distance.\n    cutoff: distance threshold (1 - similarity)\n    \"\"\"\n    n = len(fps)\n    dists = []\n    \n    for i in tqdm(range(1, n), desc=\"Computing Tanimoto distances\"):\n        sims = DataStructs.BulkTanimotoSimilarity(fps[i], fps[:i])\n        dists.extend([1.0 - s for s in sims])\n    \n    clusters = Butina.ClusterData(dists, nPts=n, distThresh=cutoff, isDistData=True)\n    \n    # Convert to cluster labels\n    labels = np.full(n, -1, dtype=int)\n    for cid, members in enumerate(clusters):\n        for idx in members:\n            labels[idx] = cid\n    \n    return labels, clusters, dists\n\n# Analyze different cutoff values for Butina clustering\nprint(\"=== Analyzing Butina Clustering Cutoffs ===\")\n\ndef butina_cutoff_analysis(fps, X, cutoffs=np.arange(0.1, 0.6, 0.05)):\n    \"\"\"\n    Analyze Butina clustering with different Tanimoto distance cutoffs.\n    \"\"\"\n    # First compute distances once\n    n = len(fps)\n    dists = []\n    for i in tqdm(range(1, n), desc=\"Computing distances\"):\n        sims = DataStructs.BulkTanimotoSimilarity(fps[i], fps[:i])\n        dists.extend([1.0 - s for s in sims])\n    \n    results = []\n    for cutoff in tqdm(cutoffs, desc=\"Testing cutoffs\"):\n        clusters = Butina.ClusterData(dists, nPts=n, distThresh=cutoff, isDistData=True)\n        \n        labels = np.full(n, -1, dtype=int)\n        for cid, members in enumerate(clusters):\n            for idx in members:\n                labels[idx] = cid\n        \n        n_clusters = len(clusters)\n        n_singletons = sum(1 for c in clusters if len(c) == 1)\n        \n        # Silhouette (only if > 1 cluster and not all singletons)\n        if n_clusters > 1 and n_clusters < n:\n            sil = silhouette_score(X, labels)\n        else:\n            sil = 0\n        \n        results.append({\n            'cutoff': cutoff,\n            'n_clusters': n_clusters,\n            'n_singletons': n_singletons,\n            'silhouette': sil\n        })\n    \n    return pd.DataFrame(results), dists\n\n# Analyze Morgan FP cutoffs\ncutoff_results_morgan, dists_morgan = butina_cutoff_analysis(\n    fps_morgan_rdkit, X_morgan.astype(np.float32)\n)\n\n# Plot cutoff analysis\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Number of clusters vs cutoff\naxes[0].plot(cutoff_results_morgan['cutoff'], cutoff_results_morgan['n_clusters'], 'bo-', linewidth=2)\naxes[0].set_xlabel('Tanimoto Distance Cutoff', fontsize=12)\naxes[0].set_ylabel('Number of Clusters', fontsize=12)\naxes[0].set_title('Butina: Clusters vs Cutoff (Morgan FP)', fontsize=14)\naxes[0].grid(True, alpha=0.3)\n\n# Singletons vs cutoff\naxes[1].plot(cutoff_results_morgan['cutoff'], cutoff_results_morgan['n_singletons'], 'go-', linewidth=2)\naxes[1].set_xlabel('Tanimoto Distance Cutoff', fontsize=12)\naxes[1].set_ylabel('Number of Singletons', fontsize=12)\naxes[1].set_title('Butina: Singletons vs Cutoff', fontsize=14)\naxes[1].grid(True, alpha=0.3)\n\n# Silhouette vs cutoff\naxes[2].plot(cutoff_results_morgan['cutoff'], cutoff_results_morgan['silhouette'], 'ro-', linewidth=2)\nbest_cutoff = cutoff_results_morgan.loc[cutoff_results_morgan['silhouette'].idxmax(), 'cutoff']\naxes[2].axvline(x=best_cutoff, color='purple', linestyle='--', label=f'Best cutoff={best_cutoff:.2f}')\naxes[2].set_xlabel('Tanimoto Distance Cutoff', fontsize=12)\naxes[2].set_ylabel('Silhouette Score', fontsize=12)\naxes[2].set_title('Butina: Silhouette vs Cutoff', fontsize=14)\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('plots/02_butina_cutoff_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nBest cutoff by silhouette: {best_cutoff:.2f}\")\nprint(cutoff_results_morgan.to_string(index=False))\n\n# Apply Butina with best cutoff\nprint(f\"\\n=== Applying Butina Clustering with cutoff={best_cutoff:.2f} ===\")\nclusters_morgan = Butina.ClusterData(dists_morgan, nPts=len(fps_morgan_rdkit), \n                                      distThresh=best_cutoff, isDistData=True)\ndf['butina_morgan'] = -1\nfor cid, members in enumerate(clusters_morgan):\n    for idx in members:\n        df.loc[idx, 'butina_morgan'] = cid\n\nprint(f\"Morgan FP clusters: {len(clusters_morgan)}\")\nprint(f\"Top 10 cluster sizes: {sorted([len(c) for c in clusters_morgan], reverse=True)[:10]}\")\n\n# Also do MACCS with same best cutoff approach\nprint(\"\\n=== MACCS Keys Clustering ===\")\ncutoff_results_maccs, dists_maccs = butina_cutoff_analysis(\n    fps_maccs_rdkit, X_maccs.astype(np.float32)\n)\nbest_cutoff_maccs = cutoff_results_maccs.loc[cutoff_results_maccs['silhouette'].idxmax(), 'cutoff']\nprint(f\"Best cutoff for MACCS: {best_cutoff_maccs:.2f}\")\n\nclusters_maccs = Butina.ClusterData(dists_maccs, nPts=len(fps_maccs_rdkit), \n                                     distThresh=best_cutoff_maccs, isDistData=True)\ndf['butina_maccs'] = -1\nfor cid, members in enumerate(clusters_maccs):\n    for idx in members:\n        df.loc[idx, 'butina_maccs'] = cid\n\nprint(f\"MACCS clusters: {len(clusters_maccs)}\")\nprint(f\"Top 10 cluster sizes: {sorted([len(c) for c in clusters_maccs], reverse=True)[:10]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Hierarchical Coarse Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_tanimoto_distance_matrix(fps):\n    \"\"\"\n    Compute full Tanimoto distance matrix.\n    \"\"\"\n    n = len(fps)\n    dist_matrix = np.zeros((n, n), dtype=np.float32)\n    \n    for i in tqdm(range(n), desc=\"Computing distance matrix\"):\n        sims = DataStructs.BulkTanimotoSimilarity(fps[i], fps)\n        dist_matrix[i, :] = [1.0 - s for s in sims]\n    \n    return dist_matrix\n\n# Compute distance matrix for Morgan fingerprints\nprint(\"Computing Tanimoto distance matrix for Morgan FPs...\")\ndist_matrix_morgan = compute_tanimoto_distance_matrix(fps_morgan_rdkit)\n\n# Compute linkage matrix for hierarchical clustering\nprint(\"Computing hierarchical linkage...\")\ncondensed = squareform(dist_matrix_morgan)\nlinkage_morgan = linkage(condensed, method='average')\nprint(\"Linkage matrix computed.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Elbow/Silhouette analysis for Hierarchical Clustering\nprint(\"Analyzing optimal cluster count for Hierarchical Clustering...\")\n\ndef hierarchical_elbow_analysis(linkage_matrix, X, k_range=range(2, 21)):\n    \"\"\"\n    Perform elbow-style analysis for hierarchical clustering.\n    Returns silhouette scores and within-cluster variance for different k values.\n    \"\"\"\n    silhouettes = []\n    inertias = []  # Within-cluster sum of squares\n    \n    for k in tqdm(k_range, desc=\"Hierarchical analysis\"):\n        labels = fcluster(linkage_matrix, k, criterion='maxclust') - 1\n        \n        # Silhouette score\n        if len(np.unique(labels)) > 1:\n            sil = silhouette_score(X, labels)\n        else:\n            sil = 0\n        silhouettes.append(sil)\n        \n        # Within-cluster sum of squares (inertia-like metric)\n        inertia = 0\n        for cluster_id in np.unique(labels):\n            cluster_points = X[labels == cluster_id]\n            centroid = cluster_points.mean(axis=0)\n            inertia += np.sum((cluster_points - centroid) ** 2)\n        inertias.append(inertia)\n    \n    return list(k_range), inertias, silhouettes\n\nk_values_hier, inertias_hier, silhouettes_hier = hierarchical_elbow_analysis(\n    linkage_morgan, X_morgan.astype(np.float32)\n)\n\n# Plot elbow curve and silhouette scores for hierarchical\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Elbow curve\naxes[0].plot(k_values_hier, inertias_hier, 'bo-', linewidth=2, markersize=8)\naxes[0].set_xlabel('Number of Clusters (k)', fontsize=12)\naxes[0].set_ylabel('Within-Cluster Sum of Squares', fontsize=12)\naxes[0].set_title('Elbow Curve - Hierarchical Clustering (Morgan FP)', fontsize=14)\naxes[0].grid(True, alpha=0.3)\n\n# Silhouette scores\naxes[1].plot(k_values_hier, silhouettes_hier, 'ro-', linewidth=2, markersize=8)\nbest_k_hier = k_values_hier[np.argmax(silhouettes_hier)]\naxes[1].axvline(x=best_k_hier, color='g', linestyle='--', label=f'Best k={best_k_hier}')\naxes[1].set_xlabel('Number of Clusters (k)', fontsize=12)\naxes[1].set_ylabel('Silhouette Score', fontsize=12)\naxes[1].set_title('Silhouette Analysis - Hierarchical Clustering', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('plots/02_hierarchical_elbow_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nBest k for Hierarchical by silhouette: {best_k_hier} (score: {max(silhouettes_hier):.4f})\")\n\n# Update hierarchical clusters with optimal k\ndf['hier_morgan'] = fcluster(linkage_morgan, best_k_hier, criterion='maxclust') - 1\nprint(f\"Hierarchical clustering with k={best_k_hier}:\")\nprint(df['hier_morgan'].value_counts().sort_index())\n\n# Optional: Plot dendrogram for visualization of hierarchy structure\nplt.figure(figsize=(15, 6))\ndendrogram(linkage_morgan, truncate_mode='lastp', p=40, leaf_rotation=90, \n           leaf_font_size=8, show_contracted=True, \n           color_threshold=0.7*max(linkage_morgan[:,2]))\nplt.axhline(y=linkage_morgan[-(best_k_hier-1), 2], color='r', linestyle='--', \n            label=f'Cut for k={best_k_hier}')\nplt.title('Hierarchical Clustering Dendrogram (Morgan FP, Tanimoto Distance)', fontsize=14)\nplt.xlabel('Sample Index / Cluster Size')\nplt.ylabel('Distance')\nplt.legend()\nplt.tight_layout()\nplt.savefig('plots/02_dendrogram_morgan.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Validation (ARI, NMI, Silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_clustering(true_labels, pred_labels, X=None, name=\"\"):\n    \"\"\"\n    Evaluate clustering quality using ARI, NMI, and optionally Silhouette.\n    \"\"\"\n    results = {\n        'Method': name,\n        'ARI': adjusted_rand_score(true_labels, pred_labels),\n        'NMI': normalized_mutual_info_score(true_labels, pred_labels)\n    }\n    \n    if X is not None:\n        # Silhouette requires at least 2 clusters\n        n_clusters = len(np.unique(pred_labels))\n        if n_clusters >= 2 and n_clusters < len(pred_labels):\n            results['Silhouette'] = silhouette_score(X, pred_labels)\n        else:\n            results['Silhouette'] = np.nan\n    \n    return results\n\nif 'polymer_class' in df.columns:\n    true_labels = df['polymer_class'].values\n    \n    validation_results = []\n    \n    # K-means on Transformer embeddings\n    validation_results.append(evaluate_clustering(\n        true_labels, df['kmeans_cluster'].values, \n        X_transformer_scaled, \"K-means (Transformer)\"\n    ))\n    \n    # Butina on Morgan FP\n    validation_results.append(evaluate_clustering(\n        true_labels, df['butina_morgan'].values, \n        X_morgan, \"Butina (Morgan FP)\"\n    ))\n    \n    # Butina on MACCS\n    validation_results.append(evaluate_clustering(\n        true_labels, df['butina_maccs'].values, \n        X_maccs, \"Butina (MACCS)\"\n    ))\n    \n    # Hierarchical on Morgan FP\n    validation_results.append(evaluate_clustering(\n        true_labels, df['hier_morgan'].values, \n        X_morgan, \"Hierarchical (Morgan FP)\"\n    ))\n    \n    validation_df = pd.DataFrame(validation_results)\n    print(\"\\n=== Clustering Validation Results ===\")\n    print(validation_df.to_string(index=False))\n    \n    # Visualize\n    fig, ax = plt.subplots(figsize=(10, 6))\n    x = np.arange(len(validation_df))\n    width = 0.25\n    \n    ax.bar(x - width, validation_df['ARI'], width, label='ARI', color='steelblue')\n    ax.bar(x, validation_df['NMI'], width, label='NMI', color='coral')\n    ax.bar(x + width, validation_df['Silhouette'].fillna(0), width, label='Silhouette', color='seagreen')\n    \n    ax.set_ylabel('Score')\n    ax.set_title('Clustering Validation: ARI / NMI / Silhouette')\n    ax.set_xticks(x)\n    ax.set_xticklabels(validation_df['Method'], rotation=30, ha='right')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('plots/03_clustering_validation.png', dpi=300, bbox_inches='tight')\n    plt.show()\nelse:\n    print(\"No polymer_class column found for validation.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Supervised Baselines (Stratified 5-Fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_cv_evaluation(X, y, name, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluate supervised classification using stratified 5-fold CV.\n",
    "    Tests multiple classifiers.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    classifiers = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=random_state, n_jobs=-1),\n",
    "        'LogisticReg': LogisticRegression(max_iter=1000, random_state=random_state, n_jobs=-1),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for clf_name, clf in classifiers.items():\n",
    "        y_pred = cross_val_predict(clf, X, y, cv=skf)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        f1_macro = f1_score(y, y_pred, average='macro')\n",
    "        f1_weighted = f1_score(y, y_pred, average='weighted')\n",
    "        \n",
    "        results.append({\n",
    "            'Representation': name,\n",
    "            'Classifier': clf_name,\n",
    "            'Accuracy': acc,\n",
    "            'F1 (macro)': f1_macro,\n",
    "            'F1 (weighted)': f1_weighted\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "if 'polymer_class' in df.columns:\n",
    "    y = df['polymer_class'].values\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    \n",
    "    print(\"Running Stratified 5-Fold CV for each representation...\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Morgan fingerprints\n",
    "    print(\"Evaluating Morgan FP...\")\n",
    "    all_results.extend(supervised_cv_evaluation(X_morgan, y_encoded, \"Morgan FP\"))\n",
    "    \n",
    "    # MACCS keys\n",
    "    print(\"Evaluating MACCS Keys...\")\n",
    "    all_results.extend(supervised_cv_evaluation(X_maccs, y_encoded, \"MACCS Keys\"))\n",
    "    \n",
    "    # RDKit descriptors + MACCS\n",
    "    print(\"Evaluating RDKit + MACCS...\")\n",
    "    all_results.extend(supervised_cv_evaluation(X_desc_maccs_scaled, y_encoded, \"RDKit + MACCS\"))\n",
    "    \n",
    "    # Transformer embeddings\n",
    "    print(\"Evaluating Transformer...\")\n",
    "    all_results.extend(supervised_cv_evaluation(X_transformer_scaled, y_encoded, \"Transformer\"))\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n=== Supervised Baseline Results (5-Fold CV) ===\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv('supervised_cv_results.csv', index=False)\n",
    "else:\n",
    "    print(\"No polymer_class column found for supervised evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize supervised results\nif 'polymer_class' in df.columns:\n    # Pivot for visualization\n    pivot_acc = results_df.pivot(index='Representation', columns='Classifier', values='Accuracy')\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    pivot_acc.plot(kind='bar', ax=ax, width=0.8)\n    ax.set_ylabel('Accuracy')\n    ax.set_title('Supervised Classification: Accuracy by Representation', fontsize=14)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n    ax.legend(title='Classifier')\n    ax.grid(True, alpha=0.3, axis='y')\n    ax.set_ylim(0, 1)\n    \n    plt.tight_layout()\n    plt.savefig('plots/04_supervised_accuracy.png', dpi=300, bbox_inches='tight')\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Representative Polymers & Structure Visualization\n",
    "\n",
    "### 6.1 Select Representative Polymers per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster_representatives(X, cluster_labels, method='centroid'):\n",
    "    \"\"\"\n",
    "    Find representative samples for each cluster.\n",
    "    \n",
    "    Methods:\n",
    "    - 'centroid': sample closest to cluster centroid\n",
    "    - 'medoid': sample with minimum total distance to other cluster members\n",
    "    \"\"\"\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    representatives = {}\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        mask = cluster_labels == cluster_id\n",
    "        cluster_indices = np.where(mask)[0]\n",
    "        X_cluster = X[mask]\n",
    "        \n",
    "        if len(cluster_indices) == 1:\n",
    "            representatives[cluster_id] = cluster_indices[0]\n",
    "            continue\n",
    "        \n",
    "        if method == 'centroid':\n",
    "            centroid = X_cluster.mean(axis=0)\n",
    "            distances = np.linalg.norm(X_cluster - centroid, axis=1)\n",
    "            rep_idx = cluster_indices[np.argmin(distances)]\n",
    "        else:  # medoid\n",
    "            # Compute pairwise distances within cluster\n",
    "            from sklearn.metrics import pairwise_distances\n",
    "            dist_matrix = pairwise_distances(X_cluster)\n",
    "            total_distances = dist_matrix.sum(axis=1)\n",
    "            rep_idx = cluster_indices[np.argmin(total_distances)]\n",
    "        \n",
    "        representatives[cluster_id] = rep_idx\n",
    "    \n",
    "    return representatives\n",
    "\n",
    "# Find representatives for K-means clusters\n",
    "print(\"Finding representative polymers for K-means clusters...\")\n",
    "reps_kmeans = find_cluster_representatives(\n",
    "    X_transformer_scaled, \n",
    "    df['kmeans_cluster'].values, \n",
    "    method='centroid'\n",
    ")\n",
    "\n",
    "print(f\"\\nRepresentative polymers (K-means):\")\n",
    "for cluster_id, idx in sorted(reps_kmeans.items()):\n",
    "    print(f\"  Cluster {cluster_id}: idx={idx}, SMILES={df.loc[idx, 'smiles_canonical'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualize Representative Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from PIL import Image as PILImage\nfrom io import BytesIO\n\ndef visualize_cluster_representatives(df, representatives, cluster_col, n_cols=4, img_size=(300, 300)):\n    \"\"\"\n    Visualize molecular structures for cluster representatives.\n    Returns a PIL Image object.\n    \"\"\"\n    mols = []\n    legends = []\n    \n    for cluster_id in sorted(representatives.keys()):\n        idx = representatives[cluster_id]\n        mol = df.loc[idx, 'mol']\n        cluster_size = (df[cluster_col] == cluster_id).sum()\n        \n        mols.append(mol)\n        legends.append(f\"Cluster {cluster_id}\\n(n={cluster_size})\")\n    \n    # Create grid image - returnPNG=True ensures we get bytes\n    img = Draw.MolsToGridImage(\n        mols, \n        molsPerRow=n_cols, \n        subImgSize=img_size,\n        legends=legends,\n        returnPNG=True\n    )\n    \n    # Convert bytes to PIL Image\n    if isinstance(img, bytes):\n        pil_img = PILImage.open(BytesIO(img))\n    else:\n        pil_img = img\n    \n    return pil_img\n\n# Visualize K-means representatives\nprint(\"Visualizing K-means cluster representatives...\")\nimg_kmeans = visualize_cluster_representatives(df, reps_kmeans, 'kmeans_cluster')\n\n# Save the image\nimg_kmeans.save('plots/10_kmeans_representatives.png')\nprint(\"Saved: plots/10_kmeans_representatives.png\")\n\n# Display\nplt.figure(figsize=(16, 12))\nplt.imshow(img_kmeans)\nplt.axis('off')\nplt.title('K-means Cluster Representatives (Transformer Embeddings)', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find and visualize hierarchical clustering representatives\nprint(\"Finding representative polymers for hierarchical clusters...\")\nreps_hier = find_cluster_representatives(\n    X_morgan.astype(np.float32), \n    df['hier_morgan'].values, \n    method='medoid'\n)\n\nprint(\"Visualizing hierarchical cluster representatives...\")\nimg_hier = visualize_cluster_representatives(df, reps_hier, 'hier_morgan')\n\n# Save the image\nimg_hier.save('plots/11_hierarchical_representatives.png')\nprint(\"Saved: plots/11_hierarchical_representatives.png\")\n\n# Display\nplt.figure(figsize=(16, 12))\nplt.imshow(img_hier)\nplt.axis('off')\nplt.title('Hierarchical Cluster Representatives (Morgan FP, Tanimoto)', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dimensionality Reduction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_umap_2d(X, metric='euclidean', n_neighbors=25, min_dist=0.2, random_state=42):\n    \"\"\"\n    Compute 2D UMAP embedding.\n    \"\"\"\n    reducer = umap.UMAP(\n        n_components=2,\n        n_neighbors=n_neighbors,\n        min_dist=min_dist,\n        metric=metric,\n        random_state=random_state\n    )\n    return reducer.fit_transform(X.astype(np.float32))\n\nprint(\"Computing UMAP embeddings...\")\nprint(\"  Morgan FP (Jaccard)...\")\nZ_morgan = compute_umap_2d(X_morgan, metric='jaccard')\nprint(\"  MACCS (Jaccard)...\")\nZ_maccs = compute_umap_2d(X_maccs, metric='jaccard')\nprint(\"  Transformer (Euclidean)...\")\nZ_transformer = compute_umap_2d(X_transformer_scaled, metric='euclidean')\nprint(\"  RDKit+MACCS (Euclidean)...\")\nZ_desc_maccs = compute_umap_2d(X_desc_maccs_scaled, metric='euclidean')\n\nprint(\"UMAP embeddings computed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_umap_comparison(embeddings_dict, color_by, title_suffix, cmap='tab10', save_path=None):\n    \"\"\"\n    Create a 2x2 comparison plot of UMAP embeddings.\n    \"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n    axes = axes.flatten()\n    \n    # Handle color array\n    color_arr = np.array(color_by).flatten()\n    \n    for ax, (name, Z) in zip(axes, embeddings_dict.items()):\n        scatter = ax.scatter(Z[:, 0], Z[:, 1], c=color_arr, s=10, alpha=0.7, cmap=cmap)\n        ax.set_title(name, fontsize=12)\n        ax.set_xlabel('UMAP-1')\n        ax.set_ylabel('UMAP-2')\n    \n    plt.colorbar(scatter, ax=axes, shrink=0.6, label=title_suffix)\n    fig.suptitle(f'Chemical Space Colored by {title_suffix}', fontsize=14)\n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\"Saved: {save_path}\")\n    \n    plt.show()\n    plt.close()\n\n# Define embeddings dictionary\nembeddings = {\n    'Morgan FP (ECFP)': Z_morgan,\n    'MACCS Keys': Z_maccs,\n    'RDKit + MACCS': Z_desc_maccs,\n    'Transformer (polyBERT)': Z_transformer\n}\n\n# Plot colored by polymer_class\nif 'polymer_class' in df.columns:\n    plot_umap_comparison(\n        embeddings, \n        df['polymer_class'].values, \n        'polymer_class',\n        save_path='plots/05_umap_by_class.png'\n    )\n\n# Plot colored by K-means cluster\nplot_umap_comparison(\n    embeddings, \n    df['kmeans_cluster'].values, \n    'K-means cluster',\n    save_path='plots/06_umap_by_kmeans.png'\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Property-colored visualizations\nprops_to_plot = ['density', 'bulk_modulus', 'thermal_conductivity', 'static_dielectric_const']\nprops_available = [p for p in props_to_plot if p in df.columns]\n\nprint(f\"Available properties for visualization: {props_available}\")\n\nif props_available:\n    for i, prop in enumerate(props_available):\n        # Get property values and handle NaN\n        prop_values = df[prop].values.copy()\n        \n        # Check for valid values\n        valid_mask = ~np.isnan(prop_values)\n        if valid_mask.sum() < 10:\n            print(f\"Skipping {prop}: too few valid values\")\n            continue\n            \n        plot_umap_comparison(\n            embeddings, \n            prop_values, \n            prop, \n            cmap='viridis',\n            save_path=f'plots/07_umap_by_{prop}.png'\n        )\nelse:\n    print(\"No property columns found for visualization.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. MACCS Keys Analysis (Upgraded Motif Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete MACCS key descriptions (all 166 keys)\nMACCS_KEY_DESCRIPTIONS = {\n    1: 'ISOTOPE',\n    2: '103 < atomic no. < 256',\n    3: 'Group IVa,Va,VIa Periods 4-6 (Ge...)',\n    4: 'Actinide',\n    5: 'Group IIIB,IVB (Sc...)',\n    6: 'Lanthanide',\n    7: 'Group VB,VIB,VIIB (V...)',\n    8: 'QAAA@1',\n    9: 'Group VIII (Fe...)',\n    10: 'Group IIa (Mg...)',\n    11: '4M Ring',\n    12: 'Group IB,IIB (Cu...)',\n    13: 'ON(C)C',\n    14: 'S-S',\n    15: 'OC(O)O',\n    16: 'QAA@1',\n    17: 'CTC',\n    18: 'Group IIIA (B...)',\n    19: '7M Ring',\n    20: 'SI',\n    21: 'C=C(Q)Q',\n    22: '3M Ring',\n    23: 'NC(O)O',\n    24: 'N-O',\n    25: 'NC(N)N',\n    26: 'C$=C($A)$A',\n    27: 'I',\n    28: 'QCH2Q',\n    29: 'P',\n    30: 'CQ(C)(C)A',\n    31: 'QX',\n    32: 'CSN',\n    33: 'NS',\n    34: 'CH2=A',\n    35: 'Group IVa,Va (N...)',\n    36: 'S Heterocycle',\n    37: 'NC(O)N',\n    38: 'NC(C)N',\n    39: 'OS(O)O',\n    40: 'S-O',\n    41: 'CTN',\n    42: 'F',\n    43: 'QHAQH',\n    44: 'OTHER',\n    45: 'C=CN',\n    46: 'BR',\n    47: 'SAN',\n    48: 'OQ(O)O',\n    49: 'CHARGE',\n    50: 'C=C(C)C',\n    51: 'CSO',\n    52: 'NN',\n    53: 'QHAAAQH',\n    54: 'QHAAQH',\n    55: 'OSO',\n    56: 'ON(O)C',\n    57: 'O Heterocycle',\n    58: 'QSQ',\n    59: 'Snot%A%A',\n    60: 'S=O',\n    61: 'AS(A)A',\n    62: 'A$A!A$A',\n    63: 'N=O',\n    64: 'A$A!S',\n    65: 'C%N',\n    66: 'CC(C)(C)A',\n    67: 'QS',\n    68: 'QHQH (&...)',\n    69: 'QQH',\n    70: 'QNQ',\n    71: 'NO',\n    72: 'OAAO',\n    73: 'S=A',\n    74: 'CH3ACH3',\n    75: 'A!N$A',\n    76: 'C=C(A)A',\n    77: 'NAN',\n    78: 'C=N',\n    79: 'NAAN',\n    80: 'NAAAN',\n    81: 'SA(A)A',\n    82: 'ACH2QH',\n    83: 'QAAAA@1',\n    84: 'NH2',\n    85: 'CN(C)C',\n    86: 'CH2QCH2',\n    87: 'X!A$A',\n    88: 'S',\n    89: 'OAAAO',\n    90: 'QHAACH2A',\n    91: 'QHAACH2Q',\n    92: 'OC(N)C',\n    93: 'QCH3',\n    94: 'QN',\n    95: 'NAAO',\n    96: '5M Ring',\n    97: 'NAAAO',\n    98: 'QAAAAA@1',\n    99: 'C=C',\n    100: 'ACH2N',\n    101: '8M Ring',\n    102: 'QO',\n    103: 'CL',\n    104: 'QHACH2A',\n    105: 'A$A($A)$A',\n    106: 'QA(Q)Q',\n    107: 'XA(A)A',\n    108: 'CH3AAACH2A',\n    109: 'ACH2O',\n    110: 'NCO',\n    111: 'NACH2A',\n    112: 'AA(A)(A)A',\n    113: 'Onot%A%A',\n    114: 'CH3CH2A',\n    115: 'CH3ACH2A',\n    116: 'CH3AACH2A',\n    117: 'NAO',\n    118: 'ACH2CH2A > 1',\n    119: 'N=A',\n    120: 'Heterocyclic atom > 1 (&...)',\n    121: 'N Heterocycle',\n    122: 'AN(A)A',\n    123: 'OCO',\n    124: 'QQ',\n    125: 'Aromatic Ring > 1',\n    126: 'A!O!A',\n    127: 'A$A!O > 1 (&...)',\n    128: 'ACH2AAACH2A',\n    129: 'ACH2AACH2A',\n    130: 'QQ > 1 (&...)',\n    131: 'QH > 1',\n    132: 'OACH2A',\n    133: 'A$A!N',\n    134: 'X (HALOGEN)',\n    135: 'Nnot%A%A',\n    136: 'O=A > 1',\n    137: 'Heterocycle',\n    138: 'QCH2A > 1 (&...)',\n    139: 'OH',\n    140: 'O > 3 (&...)',\n    141: 'CH3 > 2 (&...)',\n    142: 'N > 1',\n    143: 'A$A!O',\n    144: 'Anot%A%Anot%A',\n    145: '6M Ring > 1',\n    146: 'O > 2',\n    147: 'ACH2CH2A',\n    148: 'AQ(A)A',\n    149: 'CH3 > 1',\n    150: 'A!A$A!A',\n    151: 'NH',\n    152: 'OC(C)C',\n    153: 'QCH2A',\n    154: 'C=O',\n    155: 'A!CH2!A',\n    156: 'NA(A)A',\n    157: 'C-O',\n    158: 'C-N',\n    159: 'O > 1',\n    160: 'CH3',\n    161: 'N',\n    162: 'Aromatic',\n    163: '6M Ring',\n    164: 'O',\n    165: 'Ring',\n    166: 'Fragments'\n}\n\n# Analyze MACCS key frequencies\nmaccs_freq = X_maccs.mean(axis=0)\ntop_keys = np.argsort(maccs_freq)[::-1][:20]\n\nprint(\"=== Top 20 Most Common MACCS Keys ===\")\nfor i, key_idx in enumerate(top_keys):\n    desc = MACCS_KEY_DESCRIPTIONS.get(key_idx, f'Key {key_idx}')\n    print(f\"{i+1}. Key {key_idx} ({desc}): {100*maccs_freq[key_idx]:.1f}% of molecules\")\n\n# Visualize MACCS key distribution\nplt.figure(figsize=(14, 6))\nplt.bar(range(len(maccs_freq)), maccs_freq, color='steelblue', alpha=0.7)\nplt.xlabel('MACCS Key Index')\nplt.ylabel('Frequency (fraction of molecules)')\nplt.title('MACCS Key Frequency Distribution Across Polymers')\nplt.tight_layout()\nplt.savefig('plots/08_maccs_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Print all MACCS keys with non-zero frequency\nprint(\"\\n=== All Active MACCS Keys in Dataset ===\")\nactive_keys = np.where(maccs_freq > 0)[0]\nfor key_idx in active_keys:\n    desc = MACCS_KEY_DESCRIPTIONS.get(key_idx, f'Key {key_idx}')\n    print(f\"Key {key_idx:3d}: {desc:30s} - {100*maccs_freq[key_idx]:.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MACCS keys per polymer class (if available)\nif 'polymer_class' in df.columns:\n    print(\"\\n=== MACCS Key Profiles per Polymer Class ===\")\n    \n    class_maccs_profiles = {}\n    classes_sorted = sorted(df['polymer_class'].unique())\n    \n    for cls in classes_sorted:\n        mask = df['polymer_class'] == cls\n        class_maccs_profiles[cls] = X_maccs[mask].mean(axis=0)\n    \n    # Heatmap of class-specific MACCS profiles\n    profile_matrix = np.array([class_maccs_profiles[cls] for cls in classes_sorted])\n    \n    # Select only keys with some variation\n    key_variance = profile_matrix.var(axis=0)\n    top_variable_keys = np.argsort(key_variance)[::-1][:30]\n    \n    fig, ax = plt.subplots(figsize=(16, 8))\n    im = ax.imshow(profile_matrix[:, top_variable_keys], aspect='auto', cmap='YlOrRd')\n    plt.colorbar(im, ax=ax, label='Key Frequency')\n    ax.set_xlabel('MACCS Key Index', fontsize=12)\n    ax.set_ylabel('Polymer Class', fontsize=12)\n    ax.set_title('MACCS Key Profiles by Polymer Class (Top 30 Variable Keys)', fontsize=14)\n    ax.set_yticks(range(len(classes_sorted)))\n    ax.set_yticklabels(classes_sorted)\n    ax.set_xticks(range(len(top_variable_keys)))\n    ax.set_xticklabels(top_variable_keys, rotation=90)\n    \n    plt.tight_layout()\n    plt.savefig('plots/09_maccs_class_heatmap.png', dpi=300, bbox_inches='tight')\n    print(\"Saved: plots/09_maccs_class_heatmap.png\")\n    plt.show()\nelse:\n    print(\"No polymer_class column - skipping class-specific MACCS analysis\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save processed data and representations\nprint(\"Saving results...\")\n\n# Save cleaned dataframe with cluster assignments\ndf_save = df.drop(columns=['mol'])  # Remove mol objects for CSV\ndf_save.to_csv('PI1070_cleaned_with_clusters.csv', index=False)\nprint(\"Saved: PI1070_cleaned_with_clusters.csv\")\n\n# Save representations as numpy arrays\nnp.save('X_morgan.npy', X_morgan)\nnp.save('X_maccs.npy', X_maccs)\nnp.save('X_rdkit_desc.npy', X_rdkit_desc)\nnp.save('X_transformer.npy', X_transformer)\nprint(\"Saved: X_morgan.npy, X_maccs.npy, X_rdkit_desc.npy, X_transformer.npy\")\n\n# Save UMAP embeddings\nnp.save('Z_morgan_umap.npy', Z_morgan)\nnp.save('Z_maccs_umap.npy', Z_maccs)\nnp.save('Z_transformer_umap.npy', Z_transformer)\nnp.save('Z_desc_maccs_umap.npy', Z_desc_maccs)\nprint(\"Saved: UMAP embeddings\")\n\n# List all saved plots\nimport glob\nplot_files = sorted(glob.glob('plots/*.png'))\nprint(f\"\\n=== All saved plots ({len(plot_files)} files) ===\")\nfor f in plot_files:\n    print(f\"  {f}\")\n\nprint(\"\\n=== All results saved successfully ===\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"POLYMER REPRESENTATION ANALYSIS - SUMMARY REPORT\")\nprint(\"=\"*60)\n\nprint(f\"\\n1. DATASET\")\nprint(f\"   - Original size: {len(df_raw)}\")\nprint(f\"   - After cleaning: {len(df)}\")\nif 'polymer_class' in df.columns:\n    print(f\"   - Polymer classes: {df['polymer_class'].nunique()}\")\n\nprint(f\"\\n2. REPRESENTATIONS\")\nprint(f\"   - Morgan FP:     {X_morgan.shape}\")\nprint(f\"   - MACCS Keys:    {X_maccs.shape}\")\nprint(f\"   - RDKit Desc:    {X_rdkit_desc.shape}\")\nprint(f\"   - Transformer:   {X_transformer.shape}\")\n\nprint(f\"\\n3. CLUSTERING\")\nprint(f\"   - K-means (Transformer): {df['kmeans_cluster'].nunique()} clusters\")\nprint(f\"   - Butina (Morgan):       {df['butina_morgan'].nunique()} clusters\")\nprint(f\"   - Butina (MACCS):        {df['butina_maccs'].nunique()} clusters\")\nprint(f\"   - Hierarchical (Morgan): {df['hier_morgan'].nunique()} clusters\")\n\nif 'polymer_class' in df.columns:\n    print(f\"\\n4. CLUSTERING VALIDATION (vs polymer_class)\")\n    try:\n        print(validation_df.to_string(index=False))\n    except NameError:\n        print(\"   Validation results not available\")\n    \n    print(f\"\\n5. BEST SUPERVISED MODELS (by Accuracy)\")\n    try:\n        best_per_rep = results_df.loc[results_df.groupby('Representation')['Accuracy'].idxmax()]\n        print(best_per_rep[['Representation', 'Classifier', 'Accuracy', 'F1 (macro)']].to_string(index=False))\n    except NameError:\n        print(\"   Supervised results not available\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Analysis Complete - Tue Jan 6, 2026\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}